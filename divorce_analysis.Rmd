---
title: "Divorce Analysis"
author: "Celeste Chen and Sarah Unbehaun"
date: "May 8, 2017"
output:
  pdf_document: default
  ---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(gridExtra)
library(plyr)
library(rpart)
library(rpart.plot)
library(Hmisc)
library(DMwR)
library(randomForest)
library(plotROC)
library(factoextra)
library(NbClust)
library(cluster)
```

```{r data, include=FALSE}
load("GSSdivorce.rda")
```

### Data
```{r missing, include=FALSE, cache=TRUE}
#Restrict to 1996-2014 and married, divorced or separated
GSS.divorce <- GSS.divorce[GSS.divorce$year>=1996, ]
GSS.divorce <- GSS.divorce[GSS.divorce$marital=="MARRIED" | GSS.divorce$marital=="DIVORCED" | GSS.divorce$marital=="SEPARATED", ]
GSS.divorce$marital <- factor(GSS.divorce$marital)

#Collapse marital status to married or split
GSS.divorce$marital2 <- "married"
GSS.divorce$marital2 [GSS.divorce$marital == "SEPARATED"] <- "split"
GSS.divorce$marital2 [GSS.divorce$marital == "DIVORCED"] <- "split"
GSS.divorce$marital2 [is.na(GSS.divorce$marital) ==T] <- NA
GSS.divorce$marital2 <- factor(GSS.divorce$marital2)
GSSnew <- GSS.divorce[!is.na(GSS.divorce$marital2),]
summary(GSSnew$marital2)

#Create categorical variable for size of place
GSSnew$sizecat <- NA
GSSnew$sizecat [GSSnew$size < 10] <- "rural"
GSSnew$sizecat [GSSnew$size >=10 & GSSnew$size <= 99] <- "small"
GSSnew$sizecat [GSSnew$size >=100 & GSSnew$size <= 999] <- "medium"
GSSnew$sizecat [GSSnew$size >=1000 & GSSnew$size <= 9999] <- "large"
GSSnew$sizecat <- factor(GSSnew$sizecat)
str(GSSnew)
```

```{r visuals test,include=FALSE}
##Opinion on Premarital Sex 
#Married vs Divorced vs Separated vs Other
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$premarsx))
colnames(tab) <- c("status", "premarital", "count")
pie_1b <- ggplot(tab, aes(status, premarital)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Opinion on premarital sex?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)

##Opinion on Pornography Laws
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$pornlaw))
colnames(tab) <- c("status", "porn", "count")
pie_2b <- ggplot(tab, aes(status, porn)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Opinion on pornography regulation?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)

##Opinion on Extramarital Sex 
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$xmarsex))
colnames(tab) <- c("status", "exmar", "count")
pie_3b <- ggplot(tab, aes(status, exmar)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Opinion on extramarital sex?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)

##Party Identification 
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$partyid))
colnames(tab) <- c("status", "party", "count")
pie_4b <- ggplot(tab, aes(status, party)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Party identification?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)

##How happy are you? 
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$happy))
colnames(tab) <- c("status", "happy", "count")
pie_5b <- ggplot(tab, aes(status, happy)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("How happy are you?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)

##Fundamentalism of Religion 
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$fund))
colnames(tab) <- c("status", "fund", "count")
pie_6b <- ggplot(tab, aes(status, fund)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Fundamentalism of religion?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)

##Self-Identified Class
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$class))
colnames(tab) <- c("status", "class", "count")
pie_7b <- ggplot(tab, aes(status, class)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Self-identified class?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)

##Work Status
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$wrkstat))
colnames(tab) <- c("status", "work", "count")
pie_8b <- ggplot(tab, aes(status, work)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Work status?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)


##Degree 
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$degree))
colnames(tab) <- c("status", "edu", "count")
pie_9b <- ggplot(tab, aes(status, edu)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Years of education?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)

      ###Histogram
        EducGraph2 <- ggplot(GSSnew, aes(x= educ,  group=marital2)) + 
            geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
            geom_text(aes( label = scales::percent(..prop..),
                           y= ..prop.. ), stat= "count", vjust = -.5) +
            labs(y = "Percent", fill="educ") +
            facet_grid(~marital2) +
            scale_y_continuous(labels = scales::percent)

##Children 
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$childs))
colnames(tab) <- c("status", "child", "count")
pie_10b <- ggplot(tab, aes(status, child)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Children?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)


#Sex Education in Schools
SexEducGraph2 <- ggplot(data=subset(GSSnew, !is.na(sexeduc)), aes(x= sexeduc,  group=marital2)) + 
                  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
                  geom_text(aes( label = scales::percent(..prop..),
                                 y= ..prop.. ), stat= "count", vjust = -.5) +
                  labs(y = "Percent", fill="sexeduc") +
                  facet_grid(~marital2) +
                  scale_y_continuous(labels = scales::percent)

##Region 
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$region))
colnames(tab) <- c("status", "region", "count")
pie_12b <- ggplot(tab, aes(status, region)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Region?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)

##Can people be trusted? 
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$trust))
colnames(tab) <- c("status", "trust", "count")
pie_13b <- ggplot(tab, aes(status, trust)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Can people be trusted?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)

      TrustGraph2 <- ggplot(GSSnew, aes(x= trust,  group=marital2)) + 
                        geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") +
                        geom_text(aes( label = scales::percent(..prop..),
                                       y= ..prop.. ), stat= "count", vjust = -.5) +
                        labs(y = "Percent", fill="trust") +
                        facet_grid(~marital2) +
                        scale_y_continuous(labels = scales::percent)

##Region at age 16
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$reg16))
colnames(tab) <- c("status", "region16", "count")
pie_20b <- ggplot(tab, aes(status, region16)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Region at age 16?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)


##Live with both parents at age 16
tab <- as.data.frame(table(GSSnew$marital2, GSSnew$family16))
colnames(tab) <- c("status", "family16", "count")
pie_21b <- ggplot(tab, aes(status, family16)) + 
              geom_point(aes(size = count), colour = "navy") + 
              theme_bw() + xlab("Marital Status?") + ylab("Live with both parents at age 16?") + scale_size_continuous(range=c(3,15)) + theme(plot.title = element_text(size = 10), legend.position="none")  + coord_fixed(ratio = 1)


##If not with parents, why?
ParentsGraph2 <- ggplot(data=subset(GSSnew, !is.na(famdif16)), aes(x= famdif16, group=marital2)) + 
                  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat="count") + geom_text(aes(label = scales::percent(..prop..), y= ..prop.. ), stat= "count", vjust = -.5) + labs(y = "Percent", fill="family") + facet_grid(~marital2) + scale_y_continuous(labels = scales::percent) + theme(axis.text.x=element_text(angle = -90, hjust = 0))
```
``` {r visuals, echo=FALSE}

```

``` {r splits, include=FALSE, cache=TRUE }
## Separate GSSnew by ballot type A, B or C

GSS.A <- subset(GSSnew, (GSSnew$version==1|GSSnew$version==4))
GSS.A <- subset(GSS.A, select = c(marital2, year, sexeduc, region, premarsx, xmarsex, partyid, fund, childs, degree, Agecat1, widowed, wrkstat, polviews, happy, class, income, reg16, family16, born, parborn, sizecat, attend, relig16, bible, satfin, abnomore, absingle, divlaw, fefam))
GSS.A.comp <- GSS.A[complete.cases(GSS.A),]
GSS.B <- subset(GSSnew, (GSSnew$version==2|GSSnew$version==5))
GSS.B <- subset(GSS.B, select = c(marital2, year, sexeduc, premarsx, region, pornlaw, partyid, fund, childs, degree, Agecat1, widowed, wrkstat, polviews, happy, trust, class, income, reg16, family16, born, parborn, sizecat, attend, relig16, bible, divlaw, xmovie, satfin, helpful, fair, satjob, consci, fefam))
GSS.B.comp <- GSS.B[complete.cases(GSS.B),]
GSS.C <- subset(GSSnew, (GSSnew$version==3|GSSnew$version==6))
GSS.C <- subset(GSS.C, select = c(year, region, xmarsex, pornlaw, partyid, fund, childs, degree, Agecat1, widowed, wrkstat, polviews, happy, trust, class, reg16, family16, born, parborn, income, fair, helpful, sizecat, attend, relig16, consci, satjob, satfin, abnomore, absingle, xmovie, marital2))
GSS.C.comp <- GSS.C[complete.cases(GSS.C),]

## Train-validate-test split for GSS Ballot A
set.seed(567)
rand <- runif(nrow(GSS.A))

trainA <- GSS.A.comp[rand>0.3,]
validA <- GSS.A.comp[rand>0.15 & rand <0.3,]
testA <- GSS.A.comp[rand<0.15,]

## Train-validate-test split for GSS Ballot B
set.seed(678)
rand <- runif(nrow(GSS.B))

trainB <- GSS.B.comp[rand>0.3,]
validB <- GSS.B.comp[rand>0.15 & rand <0.3,]
testB <- GSS.B.comp[rand<0.15,]

## Train-validate-test split for GSS Ballot B
set.seed(789)
rand <- runif(nrow(GSS.C))

trainC <- GSS.C[rand>0.3,]
validC <- GSS.C[rand>0.15 & rand <0.3,]
testC <- GSS.C[rand<0.15,]
```


### Step Four: Analysis, logistic regression
``` {r logit, include=FALSE, cache=TRUE}
###Logistic Regression 
#glm(marital2 ~ <x>, data = <data>, family = binomial())
fitA <- glm(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satjob + satfin + abnomore + absingle + divlaw, family=binomial(link='logit'),data=trainA) 
summary(fitA)
trainA$pred <- fitA$fitted.values
trainA$pred <- ifelse(trainA$pred > 0.5,1,0)
table(trainA$marital2, trainA$pred)
validA$pred <- predict(fitA, newdata = validA, type = "response")
validA$pred <- ifelse(validA$pred > 0.5,1,0)
table(validA$marital2, validA$pred)
fitA2 <- glm(marital2 ~ year + region + childs + degree + Agecat1 + widowed + wrkstat + class + income +  family16 + born + parborn + sizecat, family=binomial(link='logit'),data=trainA) 
trainA$pred2 <- fitA2$fitted.values
trainA$pred2 <- ifelse(trainA$pred2 > 0.5,1,0)
table(trainA$marital2, trainA$pred2)
validA$pred2 <- predict(fitA2, newdata = validA, type = "response")
validA$pred2 <- ifelse(validA$pred2 > 0.5,1,0)
table(validA$marital2, validA$pred2)


fitB <- glm(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + size + attend + relig16 + bible + helpful + fair + consci + satjob + satfin + xmovie + divlaw + fefam, family=binomial(link='logit'),data=trainB) 
summary(fitB)

fitC <- glm(marital2 ~ year + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + size + attend + relig16 + bible + helpful + fair + consci + satjob + satfin + xmovie + divlaw, family=binomial(link='logit'),data=trainB) 
summary(fitC)

#TrainA
fitA <- glm(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + size + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam, family=binomial(link='logit'),data=trainA) 
#data = dfTraining, family = binomial())
summary(fitA)
#TrainB
fitB <- glm(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + educ + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + size + attend + relig16 + bible + helpful + fair + consci + satjob + satfin + divlaw + xmovie + fefam, family=binomial(link='logit'),data=trainB) 
summary(fitB)
#TrainC
fitC <- glm(marital2 ~ year + region + xmarsex + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + educ + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + size + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle , family=binomial(link='logit'),data=trainC) 
summary(fitC)
#eliminated fefam b/c otherwise "algorithm did not converge -- fitted probabilities numerically 0 or 1 occurred"
#issues with marital2 being only 1 value --> create a factor variable rather than character (0 = married, 1 = divorced/split?)

```

# Step Five: Decision Trees with Complete Cases
``` {r trees}

###Decision Trees 

#Ballot A
rtree_fitA <-rpart(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + size + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam, data = trainA, method = 'class', cp = 0.00479512)
 printcp(rtree_fitA)
rpart.plot(rtree_fitA, shadow.col="gray", nn=TRUE)

trainApred <- predict(rtree_fitA, trainA, type = 'class')
table(trainApred, trainA$marital2)
##Accuracy: (2626 + 407)/(2626 + 407 + 740 + 136)
###Accuracy: 77.59%

#Ballot B
rtree_fitB <-rpart(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + educ + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + size + attend + relig16 + bible + helpful + fair + consci + satjob + satfin + divlaw + xmovie + fefam, data = trainB, method = 'class', cp = 0.00519031)
printcp(rtree_fitB)
rpart.plot(rtree_fitB, shadow.col="gray", nn=TRUE)
trainBpred <- predict(rtree_fitB, trainB, type = 'class')
table(trainBpred, trainB$marital2)
##Accuracy: (2662 + 403)/(2662+403 + 753+159)
###Accuracy: 77.07%


#Ballot C
rtree_fitC <-rpart(marital2 ~ year + region + xmarsex + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + educ + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + size + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle + xmovie, data = trainC, method = 'class', cp = 0.00604491)
printcp(rtree_fitC)
rpart.plot(rtree_fitC, shadow.col="gray", nn=TRUE)
trainCpred <- predict(rtree_fitC, trainC, type = 'class')
table(trainCpred, trainC$marital2)
##Accuracy: (2633 + 468)/(2633+468+690+210)
##Accuracy: 77.51%


##Source: https://www.r-bloggers.com/using-decision-trees-to-predict-infant-birth-weights/
####Decision trees: more error in predicting married (2/3), more accurate with predicting split (4/5)
```

#Step 7: Imputing Ballots A, B and C and Train/Validate/Test Split

```{r knn imputation, include=FALSE, cache=TRUE}
#using DMwR package
## Ballot A
knnOutputA <- knnImputation(GSS.A[, !names(GSS.A) %in% "medv"])  # perform knn imputation.
anyNA(knnOutputA) 
summary(knnOutputA)

## Ballot B
knnOutputB <- knnImputation(GSS.B[, !names(GSS.B) %in% "medv"])  # perform knn imputation.
anyNA(knnOutputB) 
summary(knnOutputB)

## Ballot C
knnOutputC <- knnImputation(GSS.C[, !names(GSS.C) %in% "medv"])  # perform knn imputation.
anyNA(knnOutputC) 
summary(knnOutputC)

##Separate into train, validate and test 
## Ballot A
set.seed(567)
rand <- runif(nrow(knnOutputA))

trainAknn <- knnOutputA[rand>0.3,]
validAknn <- knnOutputA[rand>0.15 & rand <0.3,]
testAknn <- knnOutputA[rand<0.15,]

## Ballot B
set.seed(678)
rand <- runif(nrow(knnOutputB))

trainBknn <- knnOutputB[rand>0.3,]
validBknn <- knnOutputB[rand>0.15 & rand <0.3,]
testBknn <- knnOutputB[rand<0.15,]

## Ballot C
set.seed(789)
rand <- runif(nrow(knnOutputC))

trainCknn <- knnOutputC[rand>0.3,]
validCknn <- knnOutputC[rand>0.15 & rand <0.3,]
testCknn <- knnOutputC[rand<0.15,]
```
``` {r imputed decision trees, include=FALSE, cache=TRUE}
###Decision Trees 

## Ballot A
rtree_fitAknn <-rpart(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam, data = trainAknn, method = 'class', cp =  0.00435920)
printcp(rtree_fitAknn)
rpart.plot(rtree_fitAknn, shadow.col="gray", nn=TRUE)
trainApredknn <- predict(rtree_fitAknn, trainAknn, type = 'class')
table(trainApredknn, trainAknn$marital2)
##Accuracy: (2584 + 514)/(2584+514+633+178) = ###Accuracy: 79.25%
##Accuracy with collapsed levels: (2651+365)/(2651+365+111+782) = 77.15%

#Ballot B
rtree_fitBknn <-rpart(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satfin + divlaw + xmovie + fefam, data = trainBknn, method = 'class', cp =  0.00519031)
#cp = 0.00692042
printcp(rtree_fitBknn)
rpart.plot(rtree_fitBknn, shadow.col="gray", nn=TRUE)
trainBpredknn <- predict(rtree_fitBknn, trainBknn, type = 'class')
table(trainBpredknn, trainBknn$marital2)
##Accuracy: (2725+408)/(2725+408+748+96) = ###Accuracy: 78.8%
##Accuracy with collapsed levels: (2722 + 274)/(2722+274+882+99)= 75.33%


#Ballot C
rtree_fitCknn <-rpart(marital2 ~ year + region + xmarsex + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle + xmovie, data = trainCknn, method = 'class', cp = 0.00388601)
printcp(rtree_fitCknn)
rpart.plot(rtree_fitCknn, shadow.col="gray", nn=TRUE)
trainCpredknn <- predict(rtree_fitCknn, trainCknn, type = 'class')
table(trainCpredknn, trainCknn$marital2)
##Accuracy: (2717+556)/(2717+556+602+126) = ##Accuracy: 82%
##Accuracy with collapsed levels: (2692+502)/(2692+502+151+656) = 79.83%


```


Original      | predicted marriage | predicted split
--------------|--------------------|----------------
married       | 1331               | 78
split         | 368                | 162

#Step 10: Run Random Forest Based on Imputed Dataset 
```{r}
####Trying Random Forest with condensed variables 
 
######Train-Test
## Train-validate-test split for GSS Ballot A
set.seed(567)
rand <- runif(nrow(knnOutputA))

trainA1 <- knnOutputA[rand>0.3,]
validA1 <- knnOutputA[rand>0.15 & rand <0.3,]
testA1 <- knnOutputA[rand<0.15,]


forestA <- randomForest(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam, data = trainA1)
summary(trainA1)
##Measuring error rate: 
forestA
## OOB estimate of  error rate: 24.51%
#Variable importance
  print(importance(forestA, type = 2))
  #Most important variables: year, sex ed, region, premarsx, xmarsex, partyid, fundamentalism, children#, 
    #degree, age ...etc. 
 plot(forestA)
#Search for most optimal number of input features
  forestA.tune <- tuneRF(trainA1[,-1], trainA1$marital2, ntreeTry = 500, 
                     mtryStart = 1, stepFactor = 2, 
                     improve = 0.001, trace = TRUE, plot = TRUE)
 
    ##lowest OOB error: mtry = 8 with an OOB error of 23.94% --> thus, 8 variables split = optimal

forestA.tune <-randomForest(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam,data=trainA1, mtry=8, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=testA1)
  
  forestA.tune
  ##Married: OOB estimate of error rate is 24.15%; married = 6.15% vs. split: 65.65%
  
    #Predict values for train set
  pred.rf.trainA <- predict(forestA.tune, newdata = trainA1, type='prob')[,2]

#Predict values for test set
  pred.rf.testA <- predict(forestA, testA1, type='prob')[,2]

  #Set up ROC inputs
  input.rfA <- rbind(data.frame(model = "trainA", d = trainA1$marital2, m = pred.rf.trainA), 
                  data.frame(model = "testA", d = testA1$marital2,  m = pred.rf.testA))
  
#Graph all three ROCs
  roc.rfA <- ggplot(input.rfA, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  +ggtitle("TrainA")

#AUC
  calc_auc(roc.rfA)
  importance(forestA)
varImpPlot(forestA)
  
 
## Train-validate-test split for GSS Ballot B
set.seed(678)
rand <- runif(nrow(knnOutputB))

trainB1 <- knnOutputB[rand>0.3,]
validB1 <- knnOutputB[rand>0.15 & rand <0.3,]
testB1 <- knnOutputB[rand<0.15,]

forestB <- randomForest(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satfin + divlaw + xmovie + fefam, data = trainB1)
summary(trainB1)
##Measuring error rate: 
forestB
## OOB estimate of  error rate: 25.57%
  ##6.13% error rate with guessing married, 73% error rate with guessing split 

#Variable importance
  print(importance(forestB, type = 2))
  #Most important variables: year, sex ed, region, premarsx, xmarsex, partyid, fundamentalism, children#, 
    #degree, age ...etc. 
 plot(forestB)
#Search for most optimal number of input features
  forestB.tune <- tuneRF(trainB1[,-1], trainB1$marital2, ntreeTry = 500, 
                     mtryStart = 1, stepFactor = 2, 
                     improve = 0.001, trace = TRUE, plot = TRUE)
 
    ##lowest OOB error: mtry = 16 with an OOB error of 18.88% --> thus, 16 variables split = optimal

  forestB.tune <-randomForest(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satfin + divlaw + xmovie + fefam,data=trainB1, mtry=16, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=testB1)
  
  forestB.tune
  ##Married: OOB estimate of error rate is 25.19%; married = 7.37% vs. split: 68.69%
  
  forestB.param <- forestB.tune[forestB.tune[, 2] == min(forestB.tune[, 2]), 1]
  #incorrect # of dimensions? 
  
  #plotROC
    #Predict values for train set
  pred.rf.trainB <- predict(forestB.tune, newdata = trainB1, type='prob')[,2]

#Predict values for test set
  pred.rf.testB <- predict(forestB, testB1, type='prob')[,2]

  #Set up ROC inputs
  input.rfB <- rbind(data.frame(model = "trainB", d = trainB1$marital2, m = pred.rf.trainB), 
                  data.frame(model = "testB", d = testB1$marital2,  m = pred.rf.testB))
  
#Graph all three ROCs
  roc.rfB <- ggplot(input.rfB, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  +ggtitle("TrainB")

#AUC
  calc_auc(roc.rfB)
  importance(forestB)
varImpPlot(forestB)



## Train-validate-test split for GSS Ballot C
set.seed(789)
rand <- runif(nrow(knnOutputC))

trainC1 <- knnOutputC[rand>0.3,]
validC1 <- knnOutputC[rand>0.15 & rand <0.3,]
testC1 <- knnOutputC[rand<0.15,]

forestC <- randomForest(marital2 ~ year + region +  pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle, data = trainC1)
summary(trainC1)
##Measuring error rate: 
forestC
## OOB estimate of  error rate: 24.09%
  ##5.42% error rate with guessing married, 69.95% error rate with guessing split 

#Variable importance
  print(importance(forestC, type = 2))
  #Most important variables: year, sex ed, region, premarsx, xmarsex, partyid, fundamentalism, children#, 
    #degree, age ...etc. 
 plot(forestC)
#Search for most optimal number of input features
  forestC.tune <- tuneRF(trainC1[,-1], trainC1$marital2, ntreeTry = 500, 
                     mtryStart = 1, stepFactor = 2, 
                     improve = 0.001, trace = TRUE, plot = TRUE)
 
    ##lowest OOB error: mtry = 8 with an OOB error of 14.3% --> thus, 8 variables split = optimal

  forestC.tune <-randomForest(marital2 ~ year + region +  pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle,data=trainC1, mtry=8, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=testC1)
  
  forestC.tune
  ##Married: OOB estimate of error rate is 23.27%; married = 6.30% vs. split: 64.94%
  
  forestC.param <- forestC.tune[forestC.tune[, 2] == min(forestC.tune[, 2]), 1]
  #incorrect # of dimensions? 
  
  #Predict values for train set
  pred.rf.trainC <- predict(forestC.tune, newdata = trainC1, type='prob')[,2]

#Predict values for test set
  pred.rf.testC <- predict(forestC, testC1, type='prob')[,2]

  #Set up ROC inputs
  input.rfC <- rbind(data.frame(model = "trainC", d = trainC1$marital2, m = pred.rf.trainC), 
                  data.frame(model = "testC", d = testC1$marital2,  m = pred.rf.testC))
  
#Graph all three ROCs
  roc.rfC <- ggplot(input.rfC, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  +ggtitle("TrainC")

#AUC
  calc_auc(roc.rfC)
  importance(forestC)
varImpPlot(forestC)
  
  
  
  
#Source: http://dataaspirant.com/2017/01/02/k-nearest-neighbor-classifier-implementation-r-scratch/ 
```

### Step Six: Analysis, clusters
```{r cluster setup, include=FALSE, cache = TRUE }
#using packages factoextra and NbClust and Cluster
# create binary matrix
gss.a.mat <- model.matrix(~0 + year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satjob + satfin + abnomore + absingle + divlaw, GSS.A.comp)

# now with B ballot
gss.b.mat <- model.matrix(~0 + year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satjob + satfin + xmovie + divlaw + fefam, GSS.B.comp)

# now with Ballot C
gss.c.mat <- model.matrix(~0 + year + region + xmarsex + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satjob + satfin + abnomore + absingle + xmovie, GSS.C.comp)
```
``` {r cluster A, include=FALSE, cache=TRUE}
#determine optimal number of clusters using method within-cluster sum of squares
# visualize as line ("elbow" method)
elbow.a <- fviz_nbclust(gss.a.mat, kmeans, method = "wss") 
# (silhouette method)
sil.plot.a <- fviz_nbclust(gss.a.mat, kmeans, method = "silhouette") 
  # optimal cluster is 2! 
# compute kmeans clusters
km.a <- kmeans(gss.a.mat, 2, 10)
# compute hierarchical clusters and check silhouette for validation 
hc.a <- eclust(gss.a.mat, "hclust", k=2, method="ward.D2", graph=FALSE)
sil.a <- hc.a$silinfo
sil.a$clus.avg.widths
  # not very good  ~0.45
# compute PAM clusters
pam.a <- pam(gss.a.mat, 2)
# visulize as silhouette plot
fviz_silhouette(pam.a)
  # similar to hierarchical

#compare clusters to marital status
table(GSS.A.comp$marital2, km.a$cluster)
table(GSS.A.comp$marital2, hc.a$cluster)
table(GSS.A.comp$marital2, pam.a$cluster)
```
```{r cluster output A, echo=FALSE, warning=FALSE, message=FALSE}
grid.arrange(elbow.a, sil.plot.a, ncol=2)
```
```{r cluster B and C, include=FALSE, cache=TRUE}
#determine optimal number of clusters using method within-cluster sum of squares
# visualize as line ("elbow" method)
elbow.b <- fviz_nbclust(gss.b.mat, kmeans, method = "wss") 
# (silhouette method)
sil.plot.b <- fviz_nbclust(gss.b.mat, kmeans, method = "silhouette") 
  # optimal cluster is 2! 
# compute kmeans clusters
km.b <- kmeans(gss.b.mat, 2, 10)
#ksil.a <- silhouette(km.a$cluster, dist(diss.a))
#head(ksil.a)
# compute hierarchical clusters and check silhouette for validation 
hc.b <- eclust(gss.b.mat, "hclust", k=2, method="complete", graph=FALSE)
sil.b <- hc.b$silinfo
sil.b$clus.avg.widths
  # not very good  ~0.47/~0.38

# compute PAM clusters
pam.b <- pam(gss.b.mat, 2)
# visulize as silhouette plot
fviz_silhouette(pam.b)
  # similar to hierarchical

#compare clusters to marital status
table(GSS.B.comp$marital2, km.b$cluster)
table(GSS.B.comp$marital2, hc.b$cluster)
table(GSS.B.comp$marital2, pam.b$cluster)


#determine optimal number of clusters using method within-cluster sum of squares
# visualize as line ("elbow" method)
fviz_nbclust(gss.c.mat, kmeans, method = "wss") 
# (silhouette method)
fviz_nbclust(gss.c.mat, kmeans, method = "silhouette") 
  # optimal cluster is 2! 
# compute kmeans clusters
km.c <- kmeans(gss.c.mat, 2, 10)
#ksil.a <- silhouette(km.a$cluster, dist(diss.a))
#head(ksil.a)
# compute hierarchical clusters and check silhouette for validation 
hc.c <- eclust(gss.c.mat, "hclust", k=2, method="complete", graph=FALSE)
sil.c <- hc.c$silinfo
sil.c$clus.avg.widths
  # not very good  ~0.28/~0.41

# compute PAM clusters
pam.c <- pam(gss.c.mat, 2)
# visulize as silhouette plot
fviz_silhouette(pam.c)
  # similar to hierarchical

#compare clusters to marital status
table(GSS.C.comp$marital2, km.c$cluster)
table(GSS.C.comp$marital2, hc.c$cluster)
table(GSS.C.comp$marital2, pam.c$cluster)
```


#Step 12: Random Forest on Complete

```{r}
######Train-Test
## Train-validate-test split for GSS Ballot A
forestAcomplete <- randomForest(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam, data = trainA)
summary(trainA)
##Measuring error rate: 
forestAcomplete
## OOB estimate of  error rate: 24.51%
#Variable importance
  print(importance(forestAcomplete, type = 2))
  #Most important variables: year, sex ed, region, premarsx, xmarsex, partyid, fundamentalism, children#, 
    #degree, age ...etc. 
 plot(forestAcomplete)
#Search for most optimal number of input features
  forestAcomplete.tune <- tuneRF(trainA[,-1], trainA$marital2, ntreeTry = 500, 
                     mtryStart = 1, stepFactor = 2, 
                     improve = 0.001, trace = TRUE, plot = TRUE)
  ##lowest OOB error: mtry = 4 with an OOB error of 24.99% --> thus, 4 variables split = optimal

forestAcomplete.tune <-randomForest(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam,data=trainA, mtry=4, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=validA)
  
  forestAcomplete.tune
  #OOB estimate of error rate is 25.21%; married = 6.39% vs. split: 68.34%
  
  #Predict values for train set
  pred.rf.trainAcomplete <- predict(forestAcomplete.tune, newdata = trainA, type='prob')[,2]

#Predict values for validating set (or test?)
  pred.rf.validAcomplete <- predict(forestAcomplete, validA, type='prob')[,2]

  #Set up ROC inputs
  input.rfAcomplete <- rbind(data.frame(model = "trainAcomplete", d = trainA$marital2, m = pred.rf.trainAcomplete), 
                  data.frame(model = "validAcomplete", d = validA$marital2,  m = pred.rf.validAcomplete))
  ##(used to be test)
  
#Graph all three ROCs
  roc.rfAcomplete <- ggplot(input.rfAcomplete, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  +ggtitle("TrainAcomplete")

#AUC
  calc_auc(roc.rfAcomplete)
  importance(forestAcomplete)
varImpPlot(forestAcomplete)
  
################################################# 
forestBcomplete <- randomForest(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satfin + divlaw + xmovie + fefam, data = trainB)
##Measuring error rate: 
forestBcomplete
## OOB estimate of  error rate: 26.85%
  ##6.23% error rate with guessing married, 72.6% error rate with guessing split 

#Variable importance
  print(importance(forestBcomplete, type = 2))
  #Most important variables: year, sex ed, region, premarsx, xmarsex, partyid, fundamentalism, children#, 
    #degree, age ...etc. 
 plot(forestBcomplete)
#Search for most optimal number of input features
  forestBcomplete.tune <- tuneRF(trainB[,-1], trainB$marital2, ntreeTry = 500, 
                     mtryStart = 1, stepFactor = 2, 
                     improve = 0.001, trace = TRUE, plot = TRUE)
 
    ##lowest OOB error: mtry = 16 with an OOB error of 25.74% --> thus, 16 variables split = optimal

  forestBcomplete.tune <-randomForest(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satfin + divlaw + xmovie + fefam,data=trainB, mtry=16, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=validB1complete) #valid instead of test group
  
  forestBcomplete.tune
  ##OOB estimate of error rate is 27.01%; married = 8.92% vs. split: 67.12%
  
  
  #Predict values for train set
  pred.rf.trainBcomplete <- predict(forestBcomplete.tune, newdata = trainB, type='prob')[,2]

#Predict values for test set (or validating set?)
  pred.rf.validBcomplete <- predict(forestBcomplete, validB1complete, type='prob')[,2]

  #Set up ROC inputs
  input.rfBcomplete <- rbind(data.frame(model = "trainB", d = trainB$marital2, m = pred.rf.trainBcomplete), 
                  data.frame(model = "validB", d = validB1complete$marital2,  m = pred.rf.validBcomplete))
  
#Graph all three ROCs
  roc.rfBcomplete <- ggplot(input.rfBcomplete, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  +ggtitle("TrainB")

#AUC
  calc_auc(roc.rfBcomplete)
  importance(forestBcomplete)
varImpPlot(forestBcomplete)


forestCcomplete <- randomForest(marital2 ~ year + region +  pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle, data = trainC)

##Measuring error rate: 
forestCcomplete
## OOB estimate of  error rate: 30.04%
##9.82% error rate with guessing married, 67.78% error rate with guessing split 

#Variable importance
  print(importance(forestCcomplete, type = 2))
  #Most important variables: year, sex ed, region, premarsx, xmarsex, partyid, fundamentalism, children#, 
    #degree, age ...etc. 
 plot(forestCcomplete)
#Search for most optimal number of input features
  forestCcomplete.tune <- tuneRF(trainC[,-1], trainC$marital2, ntreeTry = 500, 
                     mtryStart = 1, stepFactor = 2, 
                     improve = 0.001, trace = TRUE, plot = TRUE)
 
    ##lowest OOB error: mtry = 4 with an OOB error of 28.1% --> thus, 4 variables split = optimal

  forestCcomplete.tune <-randomForest(marital2 ~ year + region +  pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle,data=trainC, mtry=4, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=validC1complete)
  
  forestCcomplete.tune
  ##OOB estimate of error rate is 29.84%; married = 8.63% vs. split: 69.44%
  
  #Predict values for train set
  pred.rf.trainCcomplete <- predict(forestCcomplete.tune, newdata = trainC, type='prob')[,2]

#Predict values for test set
  pred.rf.validCcomplete <- predict(forestCcomplete, validC1complete, type='prob')[,2]

  #Set up ROC inputs
  input.rfCcomplete <- rbind(data.frame(model = "trainCcomplete", d = trainC$marital2, m = pred.rf.trainCcomplete), 
                  data.frame(model = "validC", d = validC1complete$marital2,  m = pred.rf.validCcomplete))
  
#Graph all three ROCs
  roc.rfCcomplete <- ggplot(input.rfC, aes(d = d, model = model, m = m, colour = model)) + 
             geom_roc(show.legend = TRUE) + style_roc()  +ggtitle("TrainCcomplete")

#AUC
  calc_auc(roc.rfCcomplete)
  importance(forestCcomplete)
varImpPlot(forestCcomplete)
  
```



#Step 12: Complete on Complete (use test) Random Forest on Complete Dataset 

```{r}

##Complete on Complete (use 'test' rather than 'validate')
## Train-validate-test split for GSS Ballot A

forestAcomplete.tune <-randomForest(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam,data=trainA, mtry=4, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=testA)
   forestAcomplete.tune
######OOB estimate of error rate is 25.17%; married = 6.52% vs. split: 67.9%
 
##Complete on Complete 
## test for GSS Ballot B

  forestBcomplete.tune <-randomForest(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satfin + divlaw + xmovie + fefam,data=trainB, mtry=16, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=testB1complete)  #test group
  
  forestBcomplete.tune
  ##OOB estimate of error rate is 26.17%; married = 8.2% vs. split: 65.93%
  
 
#######Ballot C
  forestCcomplete.tune <-randomForest(marital2 ~ year + region + xmarsex + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle + xmovie,data=trainC, mtry=4, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=testC)
  
  forestCcomplete.tune
  ##OOB estimate of error rate is 29.84%; married = 8.63% vs. split: 69.44%
  

```



#Step 13: Imputed on Complete, Compare results to Complete on Complete
```{r}

##Imputed on Complete

###BallotA
forestAimponcomp.tune <-randomForest(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam,data=trainA1, mtry=8, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=testA)
  forestAimponcomp.tune
###Trained on imputed dataset, tested on complete dataset
  ##OOB estimate of error rate is 23.97%; married = 7.2% vs. split: 64.3%
  ##Imputed (train on imputed trainA1 and tested on testA) is more accurate for Ballot A

###BallotB
forestBimponcomp.tune <-randomForest(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satfin + divlaw + xmovie + fefam,data=trainB1, mtry=16, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=testB1complete)  #test group
  forestBimponcomp.tune
 ##OOBestimate of error rate is 25.4%; married = 7.62% vs. split: 68.77%
  ##Imputed is more accurate overall for Ballot B, but for split category specifically, the forestBcomplete.tune is more accurate (than imputed) for Ballot B
  
###BallotC 
   forestCimponcomp.tune <-randomForest(marital2 ~ year + region +  pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle,data=trainC1, mtry=4, ntree=500, 
     keep.forest=TRUE, importance=TRUE,test=testC)
     forestCimponcomp.tune
  
     ##OOB  estimate of error rate: 23.92%: married = 4.78%; split: 70.9%
     ##Imputed is more accurate overall for Ballot C,but for the split category speficifically, training on the complete (forestCcomplete.tune) is more accurate 
   
```


#Step 14: Decision Trees: Which one is best? 
```{r}
#Decision tree model: already dropped missing so orig is from complete, just did imputed 
#now test both on complete dataset 

#Ballot A
###Complete on Complete  
    rtree_fitAcc <-rpart(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam, data = trainA, method = 'class', cp = 0.00739645)
     printcp(rtree_fitAcc)
    rpart.plot(rtree_fitAcc, shadow.col="gray", nn=TRUE)
    
    trainApredcc <- predict(rtree_fitAcc, testA, type = 'class')
    table(trainApredcc, testA$marital2)
    ##Overall Accuracy: (337+43)/(337+43+89+24)= 77.08%
    ##Married accuracy: 337/(337+89) = 79.12%
    ##Split accuracy:  43/(24+43) = 64.18%

##Imputed on Complete
  rtree_fitAknn <-rpart(marital2 ~ year + sexeduc + region + premarsx + xmarsex + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + satfin + abnomore + absingle + divlaw + fefam, data = trainAknn, method = 'class', cp =  0.00435920)
     printcp(rtree_fitAknn)
    rpart.plot(rtree_fitAknn, shadow.col="gray", nn=TRUE)
    trainApredknnonc <- predict(rtree_fitAknn, testA, type = 'class')
    table(trainApredknnonc, testA$marital2)
    ##Accuracy: (341+35)/(341+35+97+20) =  76.27%
    ##Married accuracy: 341/(341+97) = 77.85%
    ##Split accuracy:  35/55 = 63.64%
    
############################
#Ballot B
#Complete on Complete        
    rtree_fitBcc <-rpart(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satfin + divlaw + xmovie + fefam, data = trainB, method = 'class', cp =  0.00681431)
    printcp(rtree_fitBcc)
    rpart.plot(rtree_fitBcc, shadow.col="gray", nn=TRUE)
    trainBpredcc <- predict(rtree_fitBcc, testB1complete, type = 'class')
    table(trainBpredcc, testB1complete$marital2)
    ##Overall Accuracy:  (249+33)/(249+86+33+31)= 70.68%
    ##Married accuracy:  249/(249+86)= 74.33
    ##Split accuracy:   33/(31+33) = 51.56%

##Imputed on Complete
  rtree_fitBknn <-rpart(marital2 ~ year + sexeduc + region + premarsx + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + happy + trust + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + bible + helpful + fair + consci + satfin + divlaw + xmovie + fefam, data = trainB1, method = 'class', cp =  0.00201845)
     printcp(rtree_fitBknn)
    rpart.plot(rtree_fitBknn, shadow.col="gray", nn=TRUE)
    trainBpredknnonc <- predict(rtree_fitBknn, testB1complete, type = 'class')
    table(trainBpredknnonc, testB1complete$marital2)
    ##Accuracy: (258+51)/(258+51+68+22) =  77.44%
    ##Married accuracy: 258/(258+68) = 79.14%
    ##Split accuracy:  51/(22+51) = 69.86%
    
#Ballot C    

  #Complete on Complete        
    rtree_fitCcc <-rpart(marital2 ~ year + region + xmarsex + pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle + xmovie, data = trainC, method = 'class', cp =  0.00681431)
    printcp(rtree_fitCcc)
    rpart.plot(rtree_fitCcc, shadow.col="gray", nn=TRUE)
    trainCpredcc <- predict(rtree_fitCcc, testC, type = 'class')
    table(trainCpredcc, testC$marital2)
    ##Accuracy: (58+11)/(58+11+28+16)= 61.06%
    ##Married accuracy:58/(58+28)= 67.44%
    ##Split accuracy:  11/(11+16)= 40.74%
    
    
#Imputed on Complete
rtree_fitCknn <-rpart(marital2 ~ year + region +  pornlaw + partyid + fund + childs + degree + Agecat1 + widowed + wrkstat + polviews + trust + happy + class + income + reg16 + family16 + born + parborn + sizecat + attend + relig16 + helpful + fair + consci + satjob + satfin + abnomore + absingle, data = trainCknn, method = 'class', cp =  0.00435920)
     printcp(rtree_fitCknn)
    rpart.plot(rtree_fitCknn, shadow.col="gray", nn=TRUE)
    trainCpredknn <- predict(rtree_fitCknn, testC, type = 'class')
    table(trainCpredknn, testC$marital2)
  ##Accuracy: (68+18)/(68+21+6+18)=76.11%
    ##Married accuracy:68/(68+21)=76.40%
    ##Split accuracy:  18/24=75%
 
    
```

